#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç:
1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ README –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
2. –í—ã—è–≤–ª—è–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
3. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø–ª–∞–Ω –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏
4. –°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
"""

from pathlib import Path
from typing import Dict, List, Set, Tuple
import re
import hashlib
from collections import defaultdict


def find_documentation_files(root_path: Path) -> List[Path]:
    """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ."""
    doc_files = []
    exclude_dirs = {'.git', '__pycache__', '.pytest_cache', 'venv', '.venv', 'node_modules', 'allure-results'}
    
    patterns = ['README*', '*.md', '*.rst', '*.txt']
    
    for pattern in patterns:
        for doc_file in root_path.rglob(pattern):
            if doc_file.is_file() and not any(excluded in doc_file.parts for excluded in exclude_dirs):
                # –ò—Å–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                if not any(x in doc_file.name.lower() for x in ['generated', 'auto', 'build', 'dist']):
                    doc_files.append(doc_file)
    
    return sorted(doc_files)


def analyze_document_content(file_path: Path) -> Dict[str, any]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    try:
        content = file_path.read_text(encoding='utf-8', errors='ignore')
    except:
        content = ""
    
    # –£–¥–∞–ª—è–µ–º markdown –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
    clean_content = re.sub(r'^#+\s*.*$', '', content, flags=re.MULTILINE)
    clean_content = re.sub(r'\s+', ' ', clean_content).strip()
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    content_hash = hashlib.md5(clean_content.encode()).hexdigest()
    
    return {
        'path': file_path,
        'size': len(content),
        'lines': len(content.splitlines()),
        'clean_content': clean_content,
        'content_hash': content_hash,
        'is_empty': len(clean_content) < 50,
        'has_code_examples': bool(re.search(r'```|`[^`]*`', content)),
        'has_installation_info': bool(re.search(r'install|setup|requirements|dependencies', content, re.IGNORECASE)),
        'has_usage_examples': bool(re.search(r'usage|example|run|execute', content, re.IGNORECASE)),
        'is_main_readme': file_path.name.lower() == 'readme.md' and len(file_path.parts) <= 2,
        'contains_deprecated': bool(re.search(r'deprecated|—É—Å—Ç–∞—Ä–µ–ª|legacy|old', content, re.IGNORECASE)),
    }


def find_duplicate_content(docs_analysis: List[Dict]) -> Dict[str, List[Path]]:
    """–ù–∞—Ö–æ–¥–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –¥—É–±–ª–∏—Ä—É—é—â–∏–º—Å—è —Å–æ–¥–µ—Ä–∂–∏–º—ã–º."""
    content_groups = defaultdict(list)
    
    for analysis in docs_analysis:
        if not analysis['is_empty']:
            content_groups[analysis['content_hash']].append(analysis['path'])
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –≥—Ä—É–ø–ø—ã —Å –¥—É–±–ª–∏—Ä—É—é—â–∏–º—Å—è —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
    duplicates = {hash_val: paths for hash_val, paths in content_groups.items() if len(paths) > 1}
    
    return duplicates


def categorize_documents(docs_analysis: List[Dict]) -> Dict[str, List[Dict]]:
    """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ç–∏–ø–∞–º."""
    categories = {
        'main_docs': [],       # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞
        'module_docs': [],     # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π
        'guide_docs': [],      # –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –∏ –≥–∞–π–¥—ã
        'deprecated_docs': [], # –£—Å—Ç–∞—Ä–µ–≤—à–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        'empty_docs': [],      # –ü—É—Å—Ç—ã–µ –∏–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        'config_docs': [],     # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    }
    
    for analysis in docs_analysis:
        path_str = str(analysis['path']).lower()
        
        if analysis['is_empty']:
            categories['empty_docs'].append(analysis)
        elif analysis['contains_deprecated']:
            categories['deprecated_docs'].append(analysis)
        elif analysis['is_main_readme']:
            categories['main_docs'].append(analysis)
        elif any(x in path_str for x in ['config', 'setup', 'install']):
            categories['config_docs'].append(analysis)
        elif any(x in path_str for x in ['guide', 'tutorial', 'how', 'example']):
            categories['guide_docs'].append(analysis)
        else:
            categories['module_docs'].append(analysis)
    
    return categories


def generate_consolidation_plan(root_path: Path, categories: Dict, duplicates: Dict) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–ª–∞–Ω –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏."""
    plan = []
    plan.append("# –ü–õ–ê–ù –ö–û–ù–°–û–õ–ò–î–ê–¶–ò–ò –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–ò\n")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_docs = sum(len(cat) for cat in categories.values())
    duplicate_count = sum(len(paths) for paths in duplicates.values()) - len(duplicates)
    
    plan.append(f"## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    plan.append(f"- –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {total_docs}")
    plan.append(f"- –î—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {duplicate_count}")
    plan.append(f"- –ü—É—Å—Ç—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(categories['empty_docs'])}")
    plan.append(f"- –£—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(categories['deprecated_docs'])}")
    plan.append("")
    
    # –î—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã
    if duplicates:
        plan.append("## üîÑ –î—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã")
        for i, (hash_val, paths) in enumerate(duplicates.items(), 1):
            plan.append(f"### –ì—Ä—É–ø–ø–∞ {i}")
            rel_paths = [str(p.relative_to(root_path)) for p in paths]
            for path in rel_paths:
                plan.append(f"- `{path}`")
            plan.append("**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –û—Å—Ç–∞–≤–∏—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π, –æ—Å—Ç–∞–ª—å–Ω—ã–µ —É–¥–∞–ª–∏—Ç—å")
            plan.append("")
    
    # –ü—É—Å—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    if categories['empty_docs']:
        plan.append("## üóëÔ∏è –ü—É—Å—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
        for analysis in categories['empty_docs']:
            rel_path = analysis['path'].relative_to(root_path)
            plan.append(f"- `{rel_path}` ({analysis['lines']} —Å—Ç—Ä–æ–∫)")
        plan.append("")
    
    # –£—Å—Ç–∞—Ä–µ–≤—à–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    if categories['deprecated_docs']:
        plan.append("## ‚ö†Ô∏è –£—Å—Ç–∞—Ä–µ–≤—à–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã")
        for analysis in categories['deprecated_docs']:
            rel_path = analysis['path'].relative_to(root_path)
            plan.append(f"- `{rel_path}` - —Ç—Ä–µ–±—É–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–ª–∏ —É–¥–∞–ª–µ–Ω–∏—è")
        plan.append("")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    plan.append("## üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ")
    plan.append("1. **–û—Å–Ω–æ–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è** - –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞ (README.md)")
    plan.append("2. **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è** - –≤ –ø–∞–ø–∫–µ docs/")
    plan.append("3. **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π** - —Ä—è–¥–æ–º —Å –∫–æ–¥–æ–º –º–æ–¥—É–ª—è")
    plan.append("4. **–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞** - –≤ docs/guides/")
    plan.append("5. **–£—Å—Ç–∞—Ä–µ–≤—à–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã** - —É–¥–∞–ª–∏—Ç—å –∏–ª–∏ –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞—Ç—å")
    
    return "\n".join(plan)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏."""
    project_root = Path(__file__).resolve().parent.parent.parent
    print(f"üìö –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤: {project_root}")
    
    # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    doc_files = find_documentation_files(project_root)
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(doc_files)} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
    
    # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
    print("\nüìñ –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
    docs_analysis = []
    for doc_file in doc_files:
        analysis = analyze_document_content(doc_file)
        docs_analysis.append(analysis)
        
        rel_path = doc_file.relative_to(project_root)
        status = "üìù" if analysis['has_code_examples'] else "üìÑ"
        if analysis['is_empty']:
            status = "üóëÔ∏è"
        elif analysis['contains_deprecated']:
            status = "‚ö†Ô∏è"
        
        print(f"   {status} {rel_path} ({analysis['lines']} —Å—Ç—Ä–æ–∫)")
    
    # –ü–æ–∏—Å–∫ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    duplicates = find_duplicate_content(docs_analysis)
    if duplicates:
        print(f"\nüîÑ –ù–∞–π–¥–µ–Ω–æ {len(duplicates)} –≥—Ä—É–ø–ø –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
    categories = categorize_documents(docs_analysis)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏
    plan_content = generate_consolidation_plan(project_root, categories, duplicates)
    plan_path = project_root / "docs_consolidation_plan.md"
    
    with open(plan_path, 'w', encoding='utf-8') as f:
        f.write(plan_content)
    
    print(f"\n‚úÖ –ü–ª–∞–Ω –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {plan_path}")
    
    # –í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print(f"\nüìà –ö—Ä–∞—Ç–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    for category, items in categories.items():
        if items:
            print(f"   ‚Ä¢ {category.replace('_', ' ').title()}: {len(items)}")
    
    if duplicates:
        print(f"   ‚Ä¢ –î—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –≥—Ä—É–ø–ø: {len(duplicates)}")


if __name__ == "__main__":
    main() 